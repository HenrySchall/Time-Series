## S√©ries Temporais
#### Objetivos:
- Analisar a origem da s√©rie
- Previs√µes futuras
- Descri√ß√£o do comportamento da s√©rie 
- Analisar perodicidade ou tend√™ncia 

#### Tipos:
- Univariada = apenas uma vari√°vel se altera ao longo do tempo
- Multivariada = mais de uma vari√°vel se altera ao longo do tempo

#### Conceitos:
S√©rie Temporal -> √© um conjunto de observa√ß√µes ordenadas no tempo ou um corte particular de um processo estoc√°stico desconhecido

#### Matematicamente: Y = Tdt + Szt + et 
- Tend√™ncia (Tdt): Mudan√ßas graduais em longo prazo (crescimento populacional).
- Sazonalidade (Szt): oscila√ß√µes de subida e de queda que sempre ocorrem em um determinado per√≠odo (maior valor da conta de energia el√©trica no inverno).
- Res√≠duos (et): apresenta movimentos ascendentes e descendentes da s√©rie ap√≥s a retirada do efeito de tend√™ncia ou sazonal (sequ√™ncia de vari√°veis aleat√≥rias).

![Figura-4-Decomposicao-da-serie-temporal-em-componentes-de-sazonalidade-de-tendencia-e](https://github.com/HenrySchall/Time-Series/assets/96027335/46bf2b49-dcb1-4153-9d66-2e57b4dc57ad)

Processo Estoc√°stico -> √© uma cole√ß√£o de vari√°veis aleat√≥rias definidas num mesmo espa√ßo de probabilidades (processo gerador de uma s√©rie de vari√°veis). A descri√ß√£o de um 
processo estoc√°stico √© feita atrav√©s de uma distribui√ß√£o de probabilidade conjunta (o que √© muito complexo de se fazer), ent√£o geralmente descrevemos ele por meio das fun√ß√µes:
- ùúá(ùë°)=ùê∏{ùëç(ùë°)} -> M√©dia 
- ùúé^2(ùë°)=ùëâùëéùëü{ùëç(ùë°)} -> Vari√¢ncia 
- ùõæ(ùë°1,ùë°2)=ùê∂ùëúùë£{ùëç(ùë°1),ùëç(ùë°2)} -> Autocovari√¢ncia

![Captura de tela 2024-07-04 180109](https://github.com/HenrySchall/Time-Series/assets/96027335/7ffc0399-4f35-4e82-ac69-8950c083c8f4)

Estacionaridade -> √© quando uma s√©rie temporal apresenta todas suas caracter√≠sticas estat√≠sticas constante ao longo do tempo
- Estacionaridade Fraca = √© quando as propriedades estatiaticas, s√£o constantes no tempo, E(x)=U, Var(x) = ùúé^2, COV(X,X-n) = k (corari√¢ncia entre observa√ß√µes em diferentes pontos no tempo depende do tempo espec√≠fico em que elas ocorreram). Na literatura, geralmente estacionalidade significa estacionalidade fraca.

- Estacionaridade Forte = tamb√©m chamada de estrita, √© quando a fun√ß√£o de probabilidade conjunta √© invariante no tempo, ou seja, as distribui√ß√µes individuais s√£o iguais para todos "ts". Com isso a covari√¢ncia depende apenas da dist√¢ncia entre as observa√ß√µes e n√£o do tempo especifico que ocorreram. 

![Imagem-2](https://github.com/HenrySchall/Time-Series/assets/96027335/6c237676-00e5-407f-bcc7-cddf6c1c4a34)

Passeio Aleat√≥rio (Random Walk) -> √© a soma de pequenas flutua√ß√µes estoc√°sticas (tend√™ncia estoc√°stica)
Matematicamente: ùëçùë° = ùëç(ùë°‚àí1)+ et

Autocorrela√ß√£o -> √© a correla√ß√£o de determinados per√≠odos anteriores com o per√≠odo atual, ou seja, o grau de depend√™ncia serial. Cada per√≠odo desse tipo de correla√ß√£o √© denominado lag (defasagem) e sua representa√ß√£o √© feita pela Fun√ß√£o de Autocorrela√ß√£o (FAC) e a Fun√ß√£o de Autocorrela√ß√£o Parcial (FACP), ambas comparam o valor presente com os valores passados da s√©rie, a diferen√ßa entre eles √© que a FAC analisa tanto a correla√ß√£o direta como a indireta, j√° a FACP apenas correla√ß√£o direta. Ent√£o podemos dizer, que a FAC v√™ a correla√ß√£o direta do m√™s de janeiro em mar√ßo e tamb√©m a correla√ß√£o indireta que o m√™s de janeiro teve em fevereiro que tamb√©m teve em mar√ßo, enquanto que a FACP apenas a correla√ß√£o de janeiro em mar√ßo. Essa an√°lise √© feita, porque √© o pressuposto essencial para se criar previs√µes eficientes de uma s√©rie.

Ru√≠do Branco (White Noise) -> √© quando o erro de uma s√©rie temporal, segue uma distribui√ß√£o normal, ou seja, um processo puramente aleat√≥rio. 
- E(Xt) = 0 
- Var(Xt) = ùúé^2

Transforma√ß√£o e Suaviza√ß√£o -> S√£o t√©cnicas que buscam deixar a s√©rie o mais pr√≥ximo poss√≠vel de uma distribui√ß√£o normal. Transformando o valor das var√°veis ou suavizando a tend√™ncia e/ou sazonaliade da s√©rie. Dentre todas as t√©cnicas existentes podemos citar:
1) Tranforma√ß√£o Log 
2) Tranforma√ß√£o Expoencial
3) Tranforma√ß√£o Box-Cox
4) Suaviza√ß√£o M√©dia M√≥vel Exponencial (MME) - Curto per√≠odo 
5) Suaviza√ß√£o por M√©dia M√≥vel Simples (MMS) - Longo per√≠odo

Diferencia√ß√£o -> A diferencia√ß√£o, busca transformar uma s√©rie n√£o estacion√°ria em estacion√°ria, por meio da diferen√ßa de dois per√≠odos consecutivos

#### Modelos das s√©ries temporais univariados:
Modelos lineares:
 - Modelos autorregressivos (AR)
 - Modelos m√©dias m√≥veis (MA)
 - Modelos autorregressivos e m√©dias m√≥veis (ARMA)
 - Modelos autorregressivos integrados e de m√©dias m√≥veis (ARIMA)
 - Modelos de longas depend√™ncias temporais ou mem√≥ria longa (ARFIMA)
 - Modelos autorregressivos integrados e de m√©dias m√≥veis com sazonalidade (SARIMA)
 
Modelos n√£o lineares:
 - Autorregressivo com limiar (TAR)
 - Autorregressivo com transi√ß√£o suave (STAR)
 - Troca de regime markoviano (MSM)
 - Redes neurais artificiais autorregressivas (AR-ANN)

Estrutura: 
- Autorregressivo (AR): indica que a vari√°vel √© regressada¬†em seus valores anteriores. 
- Integrado (I): indica que os valores de dados foram substitu√≠dos com a diferen√ßa entre seus valores e os valores anteriores (diferencia√ß√£o).
- M√©dia m√≥vel (MA): Indica que o erro de regress√£o √© uma combina√ß√£o linear¬†dos termos de erro dos valores passados.

Codifica√ß√£o: (p, d, q) Par√¢metro d s√≥ pode ser inteiro, caso estivessemos trabalhando com um Modelo ARFIMA, o par√¢metro d pode ser fracionado

- p = ordem da autorregress√£o.
- d = grau de diferencia√ß√£o.
- q = ordem da m√©dia m√≥vel.

Quando adicionamos a sazonalidade, al√©m da codifica√ß√£o Arima (p, d, q), incluimos a codifica√ß√£o para a Sazonalidade (P, D, Q). Ent√£o um modelo SARIMA √© definido por: (p, d, q)(P, D, Q)

Exemplos:
- Modelo ARFIMA: (1, 0.25, 1) 
- Modelo ARIMA: (2, 1, 1)
- Modelo AR: (1, 0, 0)
- Modelo MA (0, 0, 3)
- Modelo I: (0, 2, 0)
- Modelo ARMA: (4, 0, 1)
- Modelo SARIMA: (1, 1, 2)(2, 0, 1)

#### Akaike‚Äôs Information Criterion (AIC) e o Bayesian Information Criterion (BIC)
Nos modelos mais avan√ßados, as fun√ß√µes de autocorrela√ß√£o e autocorrela√ß√£o parcial n√£o s√£o informativas para definir a ordem dos modelos, por isso usasse um crit√©rio de informa√ß√£o. Um crit√©rio de informa√ß√£o √© uma forma de encontrar o n√∫mero ideal de par√¢metros de um modelo, para entend√™-lo, tenha em mente que, a cada regressor adicional, a soma dos res√≠duos n√£o vai aumentar; frequentemente, diminuir√°. A redu√ß√£o se d√° √† custa de mais regressores. Para balancear a redu√ß√£o dos erros e o aumento do n√∫mero de regressores, o crit√©rio de informa√ß√£o associa uma penalidade a esse aumento. Sendo assim, sua equa√ß√£o apresenta duas partes: a primeira mede a qualidade do ajuste do modelo aos dados, enquanto a segunda parte √© chamada de fun√ß√£o de penaliza√ß√£o dado que penaliza modelos com muitos par√¢metros, sendo assim, dado todas as combina√ß√µes de modelos procuramos aquele que apresenta menor AIC.


















