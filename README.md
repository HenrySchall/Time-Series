## S√©ries Temporais
#### Objetivos:
- Analisar a origem da s√©rie
- Previs√µes futuras
- Descri√ß√£o do comportamento da s√©rie 
- Analisar perodicidade ou tend√™ncia 

#### Tipos:
- Univariada = apenas uma vari√°vel se altera ao longo do tempo
- Multivariada = mais de uma vari√°vel se altera ao longo do tempo

#### Conceitos:
S√©rie Temporal -> √© um conjunto de observa√ß√µes ordenadas no tempo ou um corte particular de um processo estoc√°stico desconhecido

#### Matematicamente: Y = Tdt + Szt + et 
- Tend√™ncia (Tdt): Mudan√ßas graduais em longo prazo (crescimento populacional).
- Sazonalidade (Szt): oscila√ß√µes de subida e de queda que sempre ocorrem em um determinado per√≠odo (maior valor da conta de energia el√©trica no inverno).
- Res√≠duos (et): apresenta movimentos ascendentes e descendentes da s√©rie ap√≥s a retirada do efeito de tend√™ncia ou sazonal (sequ√™ncia de vari√°veis aleat√≥rias).

![Figura-4-Decomposicao-da-serie-temporal-em-componentes-de-sazonalidade-de-tendencia-e](https://github.com/HenrySchall/Time-Series/assets/96027335/46bf2b49-dcb1-4153-9d66-2e57b4dc57ad)

Processo Estoc√°stico -> √© uma cole√ß√£o de vari√°veis aleat√≥rias definidas num mesmo espa√ßo de probabilidades (processo gerador de uma s√©rie de vari√°veis). A descri√ß√£o de um 
processo estoc√°stico √© feita atrav√©s de uma distribui√ß√£o de probabilidade conjunta (o que √© muito complexo de se fazer), ent√£o geralmente descrevemos ele por meio das fun√ß√µes:
- $ùúá(ùë°)=ùê∏{ùëç(ùë°)}$ -> M√©dia 
- $ùúé^2(ùë°)=ùëâùëéùëü{ùëç(ùë°)}$ -> Vari√¢ncia 
- $ùõæ(ùë°1,ùë°2)=ùê∂ùëúùë£{ùëç(ùë°1),ùëç(ùë°2)}$ -> Autocovari√¢ncia

![Captura de tela 2024-07-04 180109](https://github.com/HenrySchall/Time-Series/assets/96027335/7ffc0399-4f35-4e82-ac69-8950c083c8f4)

Estacionaridade -> √© quando uma s√©rie temporal apresenta todas suas caracter√≠sticas estat√≠sticas constante ao longo do tempo
- Estacionaridade Fraca = √© quando as propriedades estatiaticas, s√£o constantes no tempo, E(x)=U, Var(x) = ùúé^2, COV(X,X-n) = k (corari√¢ncia entre observa√ß√µes em diferentes pontos no tempo depende do tempo espec√≠fico em que elas ocorreram). Na literatura, geralmente estacionalidade significa estacionalidade fraca.

- Estacionaridade Forte = tamb√©m chamada de estrita, √© quando a fun√ß√£o de probabilidade conjunta √© invariante no tempo, ou seja, as distribui√ß√µes individuais s√£o iguais para todos "ts". Com isso a covari√¢ncia depende apenas da dist√¢ncia entre as observa√ß√µes e n√£o do tempo especifico que ocorreram. 

![Imagem-2](https://github.com/HenrySchall/Time-Series/assets/96027335/6c237676-00e5-407f-bcc7-cddf6c1c4a34)

Passeio Aleat√≥rio (Random Walk) -> √© a soma de pequenas flutua√ß√µes estoc√°sticas (tend√™ncia estoc√°stica)
Matematicamente: $ùëçùë° = ùëç(ùë°‚àí1)+ et$

Autocorrela√ß√£o -> √© a correla√ß√£o de determinados per√≠odos anteriores com o per√≠odo atual, ou seja, o grau de depend√™ncia serial. Cada per√≠odo desse tipo de correla√ß√£o √© denominado lag (defasagem) e sua representa√ß√£o √© feita pela Fun√ß√£o de Autocorrela√ß√£o (FAC) e a Fun√ß√£o de Autocorrela√ß√£o Parcial (FACP), ambas comparam o valor presente com os valores passados da s√©rie, a diferen√ßa entre eles √© que a FAC analisa tanto a correla√ß√£o direta como a indireta, j√° a FACP apenas correla√ß√£o direta. Ent√£o podemos dizer, que a FAC v√™ a correla√ß√£o direta do m√™s de janeiro em mar√ßo e tamb√©m a correla√ß√£o indireta que o m√™s de janeiro teve em fevereiro que tamb√©m teve em mar√ßo, enquanto que a FACP apenas a correla√ß√£o de janeiro em mar√ßo. Essa an√°lise √© feita, porque √© o pressuposto essencial para se criar previs√µes eficientes de uma s√©rie.

Ru√≠do Branco (White Noise) -> √© quando o erro de uma s√©rie temporal, segue uma distribui√ß√£o normal, ou seja, um processo puramente aleat√≥rio. 
- $E(Xt) = 0$ 
- $Var(Xt) = ùúé^2$

Transforma√ß√£o e Suaviza√ß√£o -> S√£o t√©cnicas que buscam deixar a s√©rie o mais pr√≥ximo poss√≠vel de uma distribui√ß√£o normal. Transformando o valor das var√°veis ou suavizando a tend√™ncia e/ou sazonaliade da s√©rie. Dentre todas as t√©cnicas existentes podemos citar:
1) Tranforma√ß√£o Log 
2) Tranforma√ß√£o Expoencial
3) Tranforma√ß√£o Box-Cox
4) Suaviza√ß√£o M√©dia M√≥vel Exponencial (MME) - Curto per√≠odo 
5) Suaviza√ß√£o por M√©dia M√≥vel Simples (MMS) - Longo per√≠odo

Diferencia√ß√£o -> A diferencia√ß√£o, busca transformar uma s√©rie n√£o estacion√°ria em estacion√°ria, por meio da diferen√ßa de dois per√≠odos consecutivos

#### Modelos das s√©ries temporais univariados:
Modelos lineares:
 - Modelos autorregressivos (AR)
 - Modelos m√©dias m√≥veis (MA)
 - Modelos autorregressivos e m√©dias m√≥veis (ARMA)
 - Modelos autorregressivos integrados e de m√©dias m√≥veis (ARIMA)
 - Modelos de longas depend√™ncias temporais ou mem√≥ria longa (ARFIMA)
 - Modelos autorregressivos integrados e de m√©dias m√≥veis com sazonalidade (SARIMA)
 
Modelos n√£o lineares:
 - Autorregressivo com limiar (TAR)
 - Autorregressivo com transi√ß√£o suave (STAR)
 - Troca de regime markoviano (MSM)
 - Redes neurais artificiais autorregressivas (AR-ANN)

Estrutura: 
- Autorregressivo (AR): indica que a vari√°vel √© regressada¬†em seus valores anteriores. 
- Integrado (I): indica que os valores de dados foram substitu√≠dos com a diferen√ßa entre seus valores e os valores anteriores (diferencia√ß√£o).
- M√©dia m√≥vel (MA): Indica que o erro de regress√£o √© uma combina√ß√£o linear¬†dos termos de erro dos valores passados.

Codifica√ß√£o: (p, d, q) Par√¢metro d s√≥ pode ser inteiro, caso estivessemos trabalhando com um Modelo ARFIMA, o par√¢metro d pode ser fracionado

- p = ordem da autorregress√£o.
- d = grau de diferencia√ß√£o.
- q = ordem da m√©dia m√≥vel.

Quando adicionamos a sazonalidade, al√©m da codifica√ß√£o Arima (p, d, q), incluimos a codifica√ß√£o para a Sazonalidade (P, D, Q). Ent√£o um modelo SARIMA √© definido por: (p, d, q)(P, D, Q)

Exemplos:
- Modelo ARFIMA: (1, 0.25, 1) 
- Modelo ARIMA: (2, 1, 1)
- Modelo AR: (1, 0, 0)
- Modelo MA (0, 0, 3)
- Modelo I: (0, 2, 0)
- Modelo ARMA: (4, 0, 1)
- Modelo SARIMA: (1, 1, 2)(2, 0, 1)
- 
#### Fun√ß√£o de Autocorrela√ß√£o (FAC) e Fun√ß√£o de Autocorrela√ß√£o Parcial (FACP)

#### Akaike‚Äôs Information Criterion (AIC) e o Bayesian Information Criterion (BIC)
Nos modelos mais avan√ßados, as fun√ß√µes de autocorrela√ß√£o e autocorrela√ß√£o parcial n√£o s√£o informativas para definir a ordem dos modelos, por isso usasse um crit√©rio de informa√ß√£o. Um crit√©rio de informa√ß√£o √© uma forma de encontrar o n√∫mero ideal de par√¢metros de um modelo, para entend√™-lo, tenha em mente que, a cada regressor adicional, a soma dos res√≠duos n√£o vai aumentar; frequentemente, diminuir√°. A redu√ß√£o se d√° √† custa de mais regressores. Para balancear a redu√ß√£o dos erros e o aumento do n√∫mero de regressores, o crit√©rio de informa√ß√£o associa uma penalidade a esse aumento. Sendo assim, sua equa√ß√£o apresenta duas partes: a primeira mede a qualidade do ajuste do modelo aos dados, enquanto a segunda parte √© chamada de fun√ß√£o de penaliza√ß√£o dado que penaliza modelos com muitos par√¢metros, sendo assim, dado todas as combina√ß√µes de modelos procuramos aquele que apresenta menor AIC.

#### Testes Estat√≠sticos

##### Teste de Kolmogorov-Smirnov
> Qualifica a m√°xima diferen√ßa absoluta entre a fun√ß√£o de distribui√ß√£o da amostra e a fun√ß√£o de distribui√ß√£o acumulada da distribui√ß√£o de refer√™ncia (geramente distribui√ß√£o normal), ou seja, ele qualifica dist√¢ncia entre duas amostras (compara√ß√£o entre elas).

- *H0: A amostra segue a distribui√ß√£o de refer√™ncia*
- *H1: A amostra n√£o segue a distribui√ß√£o de refer√™ncia*

##### Teste de Anderson-Darling 
> Testa se uma fun√ß√£o de distribui√ß√£o acumulada f(x), pode ser candidata a ser um fun√ß√£o de distribui√ß√£o acumulada de uma amostra aleat√≥ria;

- *H0: A amostra tem distribui√ß√£o de f(x)*
- *H1: A amostra n√£o tem distribui√ß√£o f(x)*

##### Teste de Shapiro Wilk 
> O teste Shapiro Wilk segue a seguinte equa√ß√£o descrita abaixo. Sendo que xi s√£o os valores da amostra ordenados, no qual valores menores que W s√£o evid√™ncias de que os dados s√£o normais.

![Captura de tela 2024-07-04 191812](https://github.com/HenrySchall/Time-Series/assets/96027335/c9789639-2602-44bb-a9f3-491b92b65310)

> J√° o termo b √© determinado pela seguinte equa√ß√£o:

![Captura de tela 2024-07-04 192115](https://github.com/HenrySchall/Time-Series/assets/96027335/c2594f21-082f-4f6d-9293-66b45b0125fb)

> onde ai s√£o constantes geradas pelas m√©dias, vari√¢ncias e covari√¢ncias das estat√≠sticas de ordem de uma amostra de tamanho n de uma distribui√ß√£o normal (tabela da normal).

Estat√≠stica de teste:
- *H0: A amostra segue uma distribui√ß√£o normal (W-obtido < W-cr√≠tico)*
- *H1: A amostra n√£o segue uma distribui√ß√£o normal (W-obtido > W-cr√≠tico)*

![Caderno sem tiÃÅtulo-3](https://github.com/HenrySchall/Time-Series/assets/96027335/b5ca7281-9797-4bef-80b1-4686e7360a4b)

##### Teste de Jarque-Bera
> Verifica se os erros s√£o um Ru√≠do Branco, ou seja, seguem uma distribui√ß√£o normal. O teste se baseia nos res√≠duos do m√©todo dos m√≠nimos quadrados. Para sua realiza√ß√£o o teste necessita dos c√°lculos da assimetria (skewness) e da curtose (kurtosis) da amostra, dado pela seguinte f√≥rmula:
 
![Captura de tela 2024-07-04 193133](https://github.com/HenrySchall/Time-Series/assets/96027335/fe76cc80-fa40-46c8-8357-7e19e49339a5)

> onde n e o n√∫mero de observa√ß√µes (ou graus de liberdade geral); S √© aassimetria da amostra; e K √© a curtose da amostra

![Captura de tela 2024-07-04 193243](https://github.com/HenrySchall/Time-Series/assets/96027335/b24d6ca3-6e20-44ed-a3d6-5004c3646bd6)

$\widehat{u3}$ e $\widehat{u4}$ s√£o as estimativas do terceiro e quarto momentos, respectivamente; $\bar{x}$ a m√©dia da amostra, e $ùúé^2$ √© a estimativa do segundo momento, a vari√¢ncia.

- *H0: res√≠duos s√£o normalmente distribu√≠dos*
- *H1: res√≠duos n√£o s√£o normalmente distribu√≠dos*
  
##### Teste de Ader√™ncia
> Este teste √© utilizado quando deseja-se validar a hip√≥tese que um conjunto de dados √© gerado por uma determinada distribui√ß√£o de probabilidade.

- *H0: segue o modelo proposto*
- *H1: n√£o segue o modelo proposto*
  
##### Teste de Indeped√™ncia
> Este teste √© utilizado quando deseja-se validar a hip√≥tese de independ√™ncia entre duas vari√°veis aleat√≥rias. Se por exemplo, existe a funl√ßao de probabilidade conjunta das duas vari√°veis aleat√≥rias, pode-se verificar se para todos os poss√≠veis valores das vari√°vies, o produto das probabilidades margianis √© igual √† probabilidade conjunto.

- *H0: as vari√°veis aleat√≥rias s√£o independentes*
- *H1: as vari√°veis aleat√≥rias n√£o s√£o independentes*

##### Teste de Homogeneidade
> Esse teste √© utilizado quando deseja-se validar a hip√≥tese de que uma vari√°vel aleat√≥ria apresenta comportamento similar, ou homog√™neo, em rela√ß√£o √†s suas v√°rias subpopula√ß√µes. Este teste apresenta a mesma mec√¢nica do Teste de Independ√™ncia, mas uma distin√ß√£o importante se refere √† forma como as amostras s√£o coletadas. No Teste de homogeneidade fixa-se o tamanho da amostra em cada uma das subpopula√ß√µes e, ent√£o, seleciona-se uma amostra de cada uma delas.

- *H0: As subpopula√ß√µes das vari√°veis aleat√≥rias s√£o homog√™neas*
- *H1: As subpopula√ß√µes das vari√°veis aleat√≥rias n√£o s√£o homog√™neas*

#### Coeficientes de Correla√ß√£o

##### Pearson 
##### Spearman 
##### Kendall 











